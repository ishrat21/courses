INFO: (LLM[run_llm]) (2024-06-28 16:51:08): Start a new LLM process: ['python', '-m', 'llama_cpp.server', '--model', './models/llama-2-7b-chat.Q5_K_M.gguf', '--host', '0.0.0.0', '--port', '8004', '--chat_format', 'chatml', '--verbose', 'False']
INFO: (LLM[run_llm]) (2024-06-28 16:51:08): PID of Started LLM Process: 48231
INFO: (LLM[run_api]) (2024-06-28 16:51:08): Start a new API process ['uvicorn', 'llm:app', '--host', '0.0.0.0', '--port', '8003']
INFO: (LLM[run_api]) (2024-06-28 16:51:08): PID of Started API Process: 48232
INFO: (LLM[terminate]) (2024-06-28 16:56:06): Terminating process: 48231
DEBUG: (httpx[load_ssl_context]) (2024-06-28 16:56:06): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 16:56:06): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 16:56:06): connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 16:56:06): connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9ddbe9d9d0>
DEBUG: (httpcore.connection[atrace]) (2024-06-28 16:56:06): start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9ddb2115b0> server_hostname='api.openai.com' timeout=900.0
DEBUG: (httpcore.connection[atrace]) (2024-06-28 16:56:06): start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9ddd3d8110>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:06): send_request_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:06): send_request_headers.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:06): send_request_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:06): send_request_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:06): receive_response_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:07): receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 28 Jun 2024 16:56:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-jm784xq78x9dnl73zhqr4lvp'), (b'openai-processing-ms', b'784'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9375'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'3.75s'), (b'x-request-id', b'req_02c00c536d9dd9d699e91a385821ffb3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=I8hPj3.kjzje57UTjoG8Jy_k_YJe4qiZBpSUZB6_3VA-1719593767-1.0.1.1-pZlTcbYiuROxvFIYD3IGOhBEm30W2MI705bLx8iaOPpIFAQ2nNiJCWAbe9jzTsVD5ntaLNs7MflUJp9dHAGXww; path=/; expires=Fri, 28-Jun-24 17:26:07 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=0v4LtWQeRH9Ov0iyAkn3AjZO71EKLbl7KGZz91b7yDk-1719593767274-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89af354efd70abc1-YYZ'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO: (httpx[_send_single_request]) (2024-06-28 16:56:07): HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:07): receive_response_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:07): receive_response_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:07): response_closed.started
DEBUG: (httpcore.http11[atrace]) (2024-06-28 16:56:07): response_closed.complete
DEBUG: (httpcore.connection[atrace]) (2024-06-28 16:56:07): close.started
DEBUG: (httpcore.connection[atrace]) (2024-06-28 16:56:07): close.complete
DEBUG: (httpx[load_ssl_context]) (2024-06-28 17:04:31): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 17:04:31): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:04:31): connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:04:31): connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9dddcca850>
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:04:31): start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9ddb2116d0> server_hostname='api.openai.com' timeout=900.0
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:04:31): start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9ddd3d8110>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:31): send_request_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:31): send_request_headers.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:31): send_request_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:31): send_request_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:31): receive_response_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:32): receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 28 Jun 2024 17:04:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-jm784xq78x9dnl73zhqr4lvp'), (b'openai-processing-ms', b'707'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9373'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'3.762s'), (b'x-request-id', b'req_901aada1a5dccabef8af45f557fdc643'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=KB.1EYyqidADahA9.queNZofpB78rJcQd9vgJ7c_G8U-1719594272-1.0.1.1-NHvP7yCncTmIfePViWCNC0U5irczzdZBTT.xg4wRTnWyfd2Mn9F8LKeuLPs5ORa02JTnDeIvECJBp3KnnTnSXw; path=/; expires=Fri, 28-Jun-24 17:34:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=tx_6fKeQHYTtHr2ek2l2FwQaRilHZ_cSMO7S8DBCyXY-1719594272799-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89af41a6ccbaac87-YYZ'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO: (httpx[_send_single_request]) (2024-06-28 17:04:32): HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:32): receive_response_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:32): receive_response_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:32): response_closed.started
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:04:32): response_closed.complete
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:04:32): close.started
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:04:32): close.complete
DEBUG: (httpx[load_ssl_context]) (2024-06-28 17:05:19): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 17:05:19): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:05:19): connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:05:19): connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9dd9106dd0>
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:05:19): start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9ddb212180> server_hostname='api.openai.com' timeout=900.0
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:05:19): start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9dd9106e10>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:19): send_request_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:19): send_request_headers.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:19): send_request_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:19): send_request_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:19): receive_response_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:20): receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 28 Jun 2024 17:05:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-jm784xq78x9dnl73zhqr4lvp'), (b'openai-processing-ms', b'793'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9357'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'3.858s'), (b'x-request-id', b'req_58945f353f92125a740f22e920ab9030'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yDOllj0OAYfjidfQNILzUpS2lhC9on.FDpuXvPFe4iM-1719594320-1.0.1.1-4d9GOThQdt4hSRYEWdIrFi0Wj7_1D45J3uonjK4DyPDBkAbfCouBApCy7SUTVSfZwOwLjsv9LgsmnYOkKhs7Gw; path=/; expires=Fri, 28-Jun-24 17:35:20 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=S9SNnHI09rspwPuy.Z.f9oYUZoHIUJcOetpx3Yg598U-1719594320861-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89af42d31e0daa95-YYZ'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO: (httpx[_send_single_request]) (2024-06-28 17:05:20): HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:20): receive_response_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:20): receive_response_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:20): response_closed.started
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:05:20): response_closed.complete
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:05:20): close.started
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:05:20): close.complete
DEBUG: (httpx[load_ssl_context]) (2024-06-28 17:07:56): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 17:07:56): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:07:56): connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:07:56): connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9dd91154d0>
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:07:56): start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9ddb2122a0> server_hostname='api.openai.com' timeout=900.0
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:07:56): start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f9dd9115510>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): send_request_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): send_request_headers.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): send_request_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): send_request_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): receive_response_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Date', b'Fri, 28 Jun 2024 17:07:56 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'vary', b'Origin'), (b'x-request-id', b'req_4a1c89fba7cab7024e218258f021e445'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=GKqIgOwgz1OZ.ntJTTVaRtvsC0IT_mKiz0m7HlxZ.l8-1719594476-1.0.1.1-KnRV1JP86RXS2gFGyuMXCu0.EDyH3GP3JiNSYHgbVHV.hSbC1_Uzgf7ij3yeuDkytfttjDbgey2UVuAXVFjxDw; path=/; expires=Fri, 28-Jun-24 17:37:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=hBvV1N75za6AkNm6XxctIiUPTI9bUgoXPRvGmX0ctQA-1719594476933-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89af46a83d86aba5-YYZ'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO: (httpx[_send_single_request]) (2024-06-28 17:07:56): HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): receive_response_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): receive_response_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): response_closed.started
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:07:56): response_closed.complete
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:07:56): close.started
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:07:56): close.complete
INFO: (LLM[terminate]) (2024-06-28 17:09:23): Terminating process: 48231
INFO: (LLM[terminate]) (2024-06-28 17:09:23): Terminating process: 48232
INFO: (LLM[run_llm]) (2024-06-28 17:09:25): Start a new LLM process: ['python', '-m', 'llama_cpp.server', '--model', './models/llama-2-7b-chat.Q5_K_M.gguf', '--host', '0.0.0.0', '--port', '8004', '--chat_format', 'chatml', '--verbose', 'False']
INFO: (LLM[run_llm]) (2024-06-28 17:09:25): PID of Started LLM Process: 48664
INFO: (LLM[run_api]) (2024-06-28 17:09:25): Start a new API process ['uvicorn', 'llm:app', '--host', '0.0.0.0', '--port', '8003']
INFO: (LLM[run_api]) (2024-06-28 17:09:25): PID of Started API Process: 48665
INFO: (LLM[terminate]) (2024-06-28 17:09:44): Terminating process: 48664
DEBUG: (httpx[load_ssl_context]) (2024-06-28 17:09:44): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 17:09:44): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:09:44): connect_tcp.started host='0.0.0.0' port=8004 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:09:44): connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fc990f6c690>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): send_request_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): send_request_headers.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): send_request_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): send_request_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): receive_response_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): receive_response_headers.failed exception=ReadError(BrokenResourceError())
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): response_closed.started
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:09:44): response_closed.complete
ERROR: (LLM[relay]) (2024-06-28 17:09:44): Error: 
DEBUG: (httpx[load_ssl_context]) (2024-06-28 17:09:59): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 17:09:59): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:09:59): connect_tcp.started host='0.0.0.0' port=8004 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:09:59): connect_tcp.failed exception=ConnectError(OSError('All connection attempts failed'))
ERROR: (LLM[relay]) (2024-06-28 17:09:59): Error: All connection attempts failed
INFO: (LLM[terminate]) (2024-06-28 17:10:09): Terminating process: 48664
INFO: (LLM[terminate]) (2024-06-28 17:10:09): Terminating process: 48665
INFO: (LLM[run_llm]) (2024-06-28 17:10:11): Start a new LLM process: ['python', '-m', 'llama_cpp.server', '--model', './models/llama-2-7b-chat.Q5_K_M.gguf', '--host', '0.0.0.0', '--port', '8004', '--chat_format', 'chatml', '--verbose', 'False']
INFO: (LLM[run_llm]) (2024-06-28 17:10:11): PID of Started LLM Process: 48687
INFO: (LLM[run_api]) (2024-06-28 17:10:11): Start a new API process ['uvicorn', 'llm:app', '--host', '0.0.0.0', '--port', '8003']
INFO: (LLM[run_api]) (2024-06-28 17:10:11): PID of Started API Process: 48688
INFO: (LLM[terminate]) (2024-06-28 17:12:21): Terminating process: 48687
INFO: (LLM[terminate]) (2024-06-28 17:12:21): Terminating process: 48688
INFO: (LLM[run_llm]) (2024-06-28 17:14:30): Start a new LLM process: ['python', '-m', 'llama_cpp.server', '--model', './models/llama-2-7b-chat.Q5_K_M.gguf', '--host', '0.0.0.0', '--port', '8004', '--chat_format', 'chatml', '--verbose', 'False']
INFO: (LLM[run_llm]) (2024-06-28 17:14:30): PID of Started LLM Process: 48803
INFO: (LLM[run_api]) (2024-06-28 17:14:30): Start a new API process ['uvicorn', 'llm:app', '--host', '0.0.0.0', '--port', '8003']
INFO: (LLM[run_api]) (2024-06-28 17:14:30): PID of Started API Process: 48804
INFO: (LLM[terminate]) (2024-06-28 17:14:50): Terminating process: 48803
INFO: (LLM[run_llm]) (2024-06-28 17:14:50): Start a new LLM process: ['python', '-m', 'llama_cpp.server', '--model', './models/tinyllama-1.1b-1t-openorca.Q6_K.gguf', '--host', '0.0.0.0', '--port', '8004', '--chat_format', 'chatml', '--verbose', 'False']
INFO: (LLM[run_llm]) (2024-06-28 17:14:50): PID of Started LLM Process: 48829
DEBUG: (httpx[load_ssl_context]) (2024-06-28 17:15:05): load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG: (httpx[load_ssl_context_verify]) (2024-06-28 17:15:05): load_verify_locations cafile='/home/headless/miniconda3/envs/project-llm/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:15:06): connect_tcp.started host='0.0.0.0' port=8004 local_address=None timeout=900.0 socket_options=None
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:15:06): connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fade0f3d450>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): send_request_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): send_request_headers.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): send_request_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): send_request_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): receive_response_headers.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 28 Jun 2024 17:15:05 GMT'), (b'server', b'uvicorn'), (b'content-length', b'341'), (b'content-type', b'application/json'), (b'openai-processing-ms', b'902'), (b'x-request-id', b'90a18ca0a8014cdf8456b5c3877c40ec')])
INFO: (httpx[_send_single_request]) (2024-06-28 17:15:06): HTTP Request: POST http://0.0.0.0:8004/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): receive_response_body.started request=<Request [b'POST']>
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): receive_response_body.complete
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): response_closed.started
DEBUG: (httpcore.http11[atrace]) (2024-06-28 17:15:06): response_closed.complete
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:15:06): close.started
DEBUG: (httpcore.connection[atrace]) (2024-06-28 17:15:06): close.complete
INFO: (LLM[terminate]) (2024-06-28 17:17:01): Terminating process: 48803
INFO: (LLM[terminate]) (2024-06-28 17:17:01): Terminating process: 48804
INFO: (LLM[run_llm]) (2024-06-28 17:26:53): Start a new LLM process: ['python', '-m', 'llama_cpp.server', '--model', './models/llama-2-7b-chat.Q5_K_M.gguf', '--host', '0.0.0.0', '--port', '8004', '--chat_format', 'chatml', '--verbose', 'False']
INFO: (LLM[run_llm]) (2024-06-28 17:26:53): PID of Started LLM Process: 48930
INFO: (LLM[run_api]) (2024-06-28 17:26:53): Start a new API process ['uvicorn', 'llm:app', '--host', '0.0.0.0', '--port', '8003']
INFO: (LLM[run_api]) (2024-06-28 17:26:53): PID of Started API Process: 48931
